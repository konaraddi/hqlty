# HN Antipattern Digest - 2026-02-22

Top 100 candidate comments scored by heuristics.
Review these for potential new antipatterns or examples of existing ones.

---

### #1 (score: 4)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: wokwokwok | **Depth**: 1 | **Signals**: 1 escalation keyword(s), 5 direct replies
**Link**: https://news.ycombinator.com/item?id=47108307

> This is the way.
> 
> The practice is:
> 
> - simple
> 
> - effective
> 
> - retains control and quality
> 
> Certainly the “unsupervised agent” workflows are getting a lot of attention right now, but they require a specific set of circumstances to be effective:
> 
> - clear validation loop (eg. Compile the kernel, here is gcc that does so correctly)
> 
> - ai enabled tooling (mcp / cli tool that will lint, test and provide feedback immediately)
> 
> - oversight to prevent sgents going off the rails (open area of research)
> 
> - an unlimited token budget
> 
> That means that most people can't use unsupervised agents.
> 
> Not that they dont work; Most people have simply not got an environment and task that is appropriate.
> 
> By comparison, anyone with cursor or claude can immediately start using this approach, or their own variant on it.
> 
> It does not require fancy tooling.
> 
> It does not require an arcane agent framework.
> 
> It works generally well across models.
> 
> This is one of those few genunie pieces of good practical advice for people getting into AI coding.
> 
> Simple. Obviously works once you start using it. No external dependencies. BYO tools to help with it, no “buy my AI startup xxx to help”. No “star my github so I can a job at $AI corp too”.
> 
> Great stuff.

---

### #2 (score: 4)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: Quothling | **Depth**: 3 | **Signals**: 1 escalation keyword(s), thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108865

**Parent comment**:
> I partly agree with you. But once you have a codebase large enough, the changes become longer to even type in, once figured out.
> 
> I find the best way to use agents (and I don't use claude) is to hash it out like I'm about to write these changes and I make my own mental notes, and get the agent to execute on it.
> 
> Agents don't get tired, they don't start fat fingering stuff at 4pm, the quality doesn't suffer. And they can be parallelised.
> 
> Finally, this allows me to stay at a higher level and n...

**This comment**:
> I think it comes down to "it depends". I work in a NIS2 regulated field and we're quite callenged by the fact that it means we can't give AI's any sort of real access because of the security risk. To be complaint we'd have to have the AI agent ask permission for every single thing it does, before it does it, and foureye review it. Which is obviously never going to happen. We can discuss how bad the NIS2 foureye requirement works in the real world another time, but considering how easy it is to break AI security, it might not be something we can actually ever use. This makes sense on some of the stuff we work on, since it could bring an entire powerplant down. On the flip-side AI risks would be of little concern on a lot of our internal tools, which are basically non-regulated and unimportant enough that they can be down for a while without costing the business anything beyond annoyances.
> 
> This is where our challenges are. We've build our own chatbot where you can "build" your own agent within the librechat framework and add a "skill" to it. I say "skill" because it's older than claude skills but does exactly the same. I don't completely buy the authors:
> 
> > “deeply”, “in great details”, “intricacies”, “go through everything”
> 
> bit, but you can obviously save a lot of time by writing a piece of english which tells it what sort of environment you work in. It'll know that when I write Python I use UV, Ruff and Pyrefly and so on as an example. I personally also have a "skill" setting that tells the AI not to compliment me because I find that ridicilously annoying, and that certainly works. So who knows? Anyway, employees are going to want more. I've been doing some PoC's running open source models in isolation on a raspberry pi (we had spares because we use them in IoT projects) but it's hard to setup an isolation policy which can't be circumvented.
> 
> We'll have to figure it out though. For powerplant critical projects we don't want to use AI. But for the web tool that allows a couple of employees to upload three excel files from an external accountant and then generate some sort of report on them? Who cares who writes it or even what sort of quality it's written with? The lifecycle of that tool will probably be something that never changes until the external account does and then the tool dies. Not that it would have necessarily been written in worse quality without AI... I mean... Have you seen some of the stuff we've written in the past 40 years?

---

### #3 (score: 4)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: someguy2026 | **Depth**: 4 | **Signals**: 1 escalation keyword(s), thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47106600

**Parent comment**:
> LLM speed is roughly <memory_bandwidth> / <model_size> tok/s.
> 
> DDR4 tops out about 27Gbs
> 
> DDR5 can do around 40Gbs
> 
> So for 70B model at 8 bit quant, you will get around 0.3-0.5 tokens per second using RAM alone.

**This comment**:
> DRAM speeds is one thing, but you should also account for the data rate of the PCIe bus (and/or VRAM speed). But yes, holding it "lukewarm" in DRAM rather than on NVMe storage is obviously faster.

---

### #4 (score: 4)

**Story**: How far back in time can you understand English?
**Author**: dddgghhbbfblk | **Depth**: 1 | **Signals**: 1 escalation keyword(s), 7 direct replies
**Link**: https://news.ycombinator.com/item?id=47102226

> Should be "how far back in time can you read English?" The language itself is what is spoken and the writing, while obviously related, is its own issue. Spelling is conventional and spelling and alphabet changes don't necessarily correspond to anything meaningful in the spoken language; meanwhile there can be large changes in pronunciation and comprehensibility that are masked by an orthography that doesn't reflect them.

---

### #5 (score: 4)

**Story**: Evidence of the bouba-kiki effect in naïve baby chicks
**Author**: ChrisClark | **Depth**: 3 | **Signals**: 1 escalation keyword(s), thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47106947

**Parent comment**:
> Okay Gemini

**This comment**:
> If you don't recognize a quote, it's obviously AI? Might want to rethink your logic, or outsource it to AI. Might help you

---

### #6 (score: 4)

**Story**: Parse, Don't Validate and Type-Driven Design in Rust
**Author**: rq1 | **Depth**: 3 | **Signals**: 1 escalation keyword(s), thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47106540

**Parent comment**:
> How does that work? If the length of the array is read from stdin for example, it would be impossible to know it at compile time. Presumably this is limited somehow?

**This comment**:
> Imagine you read a value from stdin and parse it as:
> 
> Maybe Int
> 
> So your program splits into two branches:
> 
> 1. Nothing branch: you failed to obtain an Int.
> 
> There is no integer to use as an index, so you can’t even attempt a safe lookup into something like Vect n a.
> 
> 2. Just i branch: you do have an Int called i.
> 
> But an Int is not automatically a valid index for Vect n a, because vectors are indexed by Fin n (a proof carrying “bounded natural”).
> 
> So inside the Just i branch, you refine further:
> 
> 3. Try to turn the runtime integer i into a value of type Fin n.
> 
> There are two typical shapes of this step:
> 
> * Checked conversion returning Maybe (Fin n)
> 
> If the integer is in range, you get Just (fin : Fin n). Otherwise Nothing.
> 
> Checked conversion returning evidence (proof) that it’s in range
> 
> For example: produce k : Nat plus a proof like k < n (or LTE (S k) n), and then you can construct Fin n from that evidence.
> 
> (But it’s the same basically, you end up with a “Maybe LTE…”
> 
> Now if you also have a vector:
> xs : Vect n a
> 
> … the n in Fin n and the n in Vect n a are the same n (that’s what “unifies” means here: the types line up), so you can do:
> index fin xs : a
> 
> And crucially:
> 
> there is no branch in which you can call index without having constructed the Fin n first,
> so out-of-bounds access is unrepresentable (it’s not “checked later”, it’s “cannot be expressed”).
> 
> And within _that_ branch of the program, you have a proof of Fin n.
> 
> Said differently: you don’t get “compile-time knowledge of i”; you get a compile-time guarantee that whatever value you ended up with satisfies a predicate.
> 
> Concretely: you run a runtime check i < n. _ONCE_
> 
> If it fails, you’re in a branch where you do not have Fin n.
> 
> If it succeeds, you construct fin : Fin n at runtime (it’s a value, you can’t get around that), but its type encodes the invariant “in bounds”/check was done somewhere in this branch.

---

### #7 (score: 4)

**Story**: zclaw: personal AI assistant in under 888 KB, running on an ESP32
**Author**: godelski | **Depth**: 6 | **Signals**: 1 escalation keyword(s), thread death at depth 6
**Link**: https://news.ycombinator.com/item?id=47107838

**Parent comment**:
> https://www.youtube.com/watch?v=VaeI9YgE1o8
> 
> Yes I know how much a kilobyte is.  But cutting down to 2 million 3 bit parameters or something like that would definitely be possible.
> 
> And a 32 bit processor should be able to pack and unpack parameters just fine.
> 
> Edit: Hey look what I just found https://github.com/DaveBben/esp32-llm "a 260K parameter tinyllamas checkpoint trained on the tiny stories dataset"

**This comment**:
> > But cutting down to 2 million 3 bit parameters or something like that would definitely be possible.
> 
> Sure, but there's no free lunch
> 
>   > Hey look what I just found
> 
> I've even personally built smaller "L"LMs. The first L is in quotes because it really isn't large (So maybe lLM?) and they aren't anything like what you'd expect and certainly not what the parent was looking for. The utility of them is really not that high... (there are special cases though) Can you "do" it? Yeah. I mean you can make a machine learning model of essentially arbitrary size. Will it be useful? Obviously that's not guaranteed. Is it fun? Yes. Is it great for leaning? Also yes.
> 
> And remember, Tiny Stories is 1GB of data. Can you train it for longer and with more data? Again, certainly, BUT again, there are costs. That Minecraft one is far more powerful than this thing.
> 
> Also, remember that these models are not RLHF'd, so you really shouldn't expect it to act like you're expecting a LLM to work. It is only at stage 0, the "pre-training", or what Karpathy calls a "babbler".

---

### #8 (score: 4)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: throwaway290 | **Depth**: 4 | **Signals**: 1 escalation keyword(s), thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108588

**Parent comment**:
> I've heard that Samsung's business practices can be quite predatory.  Basically if you have cool tech and you try to sell it to Samsung, you'll often get a few meetings and then they will go silent and then what you were trying to sell them will be offered by them as a new product about a year later.  At least this was the situation like a decade ago.
> 
> I think this is because they are a huge conglomerate and there are divisions and groups that specialize in everything and their (Samsung) cult...

**This comment**:
> > you'll often get a few meetings and then they will go silent and then what you were trying to sell them will be offered by them as a new product about a year later
> 
> This is how business works anywhere. there are no charities. whatever you say to investors or suppliers they can use  so you better be careful have lawyers and set up correctly.
> 
> (The caveat is of course when Chinese companies do this your lawyers can do nothing while in a developed country you can have some recourse)
> 
> But even if Samsung was super predatory business wise it is beside the point. if they both get subsidies and de facto are close to their governments then you have to look at what their governments do. If you like what CCP is doing, it's your choice

---

### #9 (score: 4)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: jiggawatts | **Depth**: 3 | **Signals**: 1 escalation keyword(s), thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47105827

**Parent comment**:
> > It is just so much simpler with electricity.
> 
> Yet the market still thinks differently. Lots of countries still keep subsidizing EV despite them already being mature technology for such a long time.
> 
> We didn't have to subsidize the smart phone to make it successful, we shouldn't have to subsidize electric cars either.

**This comment**:
> > we shouldn't have to subsidize electric cars either.
> 
> Smart phones were subsidised, just less obviously. Much of the fundamental research into the radio systems was done by government labs, for example.
> 
> Not to mention that governments provide maaaaasssive subsidies to the entire fossil fuel industry, including multi-trillion dollar wars in the middle east to control the oil!
> 
> Look at it from the perspective of pollution control in cities. China just invested tens of billions - maybe hundreds — into clearing out the smog they were notorious for. Electric vehicles are a part of the solution.
> 
> The alternative is everyone living a decade less because… the market forces will it.

---

### #10 (score: 4)

**Story**: A16z partner says that the theory that we’ll vibe code everything is wrong
**Author**: copperx | **Depth**: 4 | **Signals**: 1 escalation keyword(s), thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108047

**Parent comment**:
> Why does AI make it cheaper to build internal but not cheaper for SaaS competitors to pop up? Everyone has access to the same tools.

**This comment**:
> It does make it cheaper, obviously. But the barrier to entry is almost zero, like panhandling. That's why it can't substitute a job.

---

### #11 (score: 4)

**Story**: What not to write on your security clearance form (1988)
**Author**: TrackerFF | **Depth**: 1 | **Signals**: 1 escalation keyword(s), 12 direct replies
**Link**: https://news.ycombinator.com/item?id=47103514

> The fact is that even for (NATO) top secret security clearances, there are lots of people that lie through their teeth, and receive the clearance. Obviously on things that aren't in any records. The big ones being alcohol use, drug use, personal finances, foreign partners. Some are more forgiving than others, though.
> 
> The military is unfortunately chock full of functional alcoholics. As long as they don't get caught drunk on the job, seen partying too much, DIU, or admit anything to their doctor, they keep getting renewed their clearance.
> 
> Interestingly enough, if there's even the smallest suspicious that you smoke weed, they'll put you through the wringer. I've seen more people lose their clearance for pissing hot, than those with six figure debts or drinking 5 days a week.

---

### #12 (score: 4)

**Story**: What not to write on your security clearance form (1988)
**Author**: godelski | **Depth**: 3 | **Signals**: 1 escalation keyword(s), thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47105363

**Parent comment**:
> imagine curing alcoholics and drug dependant ppl who work for you ?
> 
> I'm really surprised at how they would rather ignore or silence all and report that they is strictly no problem among their pool of employees, to say they have the best employees and good KPIs
> 
> It doesn't look like a winning strategy indeed.
> 
> I myself refused to do government jobs as the table in which you had to list foreigners in your friend list was just so small. They prefer you to say you don't know nobody.
> 
> Also yeah, ...

**This comment**:
> > imagine curing alcoholics and drug dependant ppl who work for you ?
> 
> To complicate this further I think people don't recognize how people can start their jobs without problems and then gain them. These are stressful jobs (and with low pay) so that itself is a common gateway to a drinking problem. But there's also very mundane ways too. A large number of heroine and fentanyl addicts had their addictions begin through use of legal medication. The problem is we have a culture that pretends addiction is a choice and that the only to become addicted is through poor decisions and that to kick an addiction just requires "really wanting to stop". But that's not really consistent with the definition of addiction...
> 
> It seems like a poor strategy for high security topics, like you say. If anything, I want these people to have zero fear of opening up about their addictions. Be it gained unintentionally or through bad decisions. Reason being that 1) it reduces the risk of blackmail and 2) giving them a pathway to help also reduces their chance of blackmail. We don't even need to mention the fact that these are people and should be treated with kindness, we have entirely selfish reasons to be selfless.
> 
>   > I myself refused to do government jobs as the table in which you had to list foreigners in your friend list was just so small.
> 
> I always found that odd myself. Do these people know what the demographics of a typical American University are these days? If you don't have a decent list of foreign nationals then you're either 1) a social recluse or 2) in a cultural bubble, and probably not the kind that we want people with this kind of authority to have... But I think they could resolve some of this by clarifying what level of contact they mean. Is it someone you sit next to in class and talk to frequently? Or do they not count if you don't talk with them outside class or study groups? Last time I looked at the forum it seems like they want you to just list anyone you ever talked to.
> 
> Personally I've avoided getting a clearance because I just don't see the value. It is a lot of work to put together, forces you to be more quiet about what you work on, means you need to be more careful/vigilant in every day things and especially when traveling, and all for what? Low pay and not even that cool of work? I mean if it was working on alien technologies and cool sci-fi shit, sign me up! But the reality is that most of the work isn't very exciting. I'd rather have more freedom, more pay, and work on more interesting things. Maybe their work can have more purpose and more impact, but I am also not convinced that's true for the majority of things you need clearance for (even as a person in STEM).

---

### #13 (score: 4)

**Story**: What not to write on your security clearance form (1988)
**Author**: ganoushoreilly | **Depth**: 4 | **Signals**: 1 escalation keyword(s), thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47106515

**Parent comment**:
> How do you really ever know if someone you hired for psyops is telling you the truth?

**This comment**:
> It gets weirder when they train you how to evade polygraphs as part of your role.. only to have you take one for your re investigation and to be asked "have you ever tried to evade a polygraph" or something along those lines. Of course you're not in a SCIF and your training or having been exposed to that training may in fact be classified. Quite the pickle..

---

### #14 (score: 3)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: r0b05 | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108005

**Parent comment**:
> It's the attention mechanism at work, along with a fair bit of Internet one-up-manship.  The LLM has ingested all of the text on the Internet, as well as Github code repositories, pull requests, StackOverflow posts, code reviews, mailing lists, etc.  In a number of those content sources, there will be people saying "Actually, if you go into the details of..." or "If you look at the intricacies of the problem" or "If you understood the problem deeply" followed by a very deep, expert-level expl...

**This comment**:
> This is such a good explanation. Thanks

---

### #15 (score: 3)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: Scrapemist | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108176

**Parent comment**:
> It’s actually really common. If you look at Claude Code’s own system prompts written by Anthropic, they’re littered with “CRITICAL (RULE 0):” type of statements, and other similar prompting styles.

**This comment**:
> Where can I find those?

---

### #16 (score: 3)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: deevus | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107168

**Parent comment**:
> If you’ve ever desired the ability for annotating the plan more visually, try fitting Plannotator in this workflow. There is a slash command for use when you use custom workflows outside of normal plan mode.
> 
> https://github.com/backnotprop/plannotator

**This comment**:
> I'll give this a try. Thanks for the suggestion.

---

### #17 (score: 3)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: 01HNNWZ0MV43FF | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108527

**Parent comment**:
> That's an interesting stress test for I2P. They should try to fix that, the protocol should be resilient to such an event. Even if there are 10x more bad nodes than good nodes (assuming they were noncompliant I2P actors based on that thread) the good nodes should still be able to find each other and continue working. To be fair spam will always be a thorny problem in completely decentralized protocols.

**This comment**:
> Finding good nodes is a thorny problem for human friendship, too!

---

### #18 (score: 3)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: margalabargala | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108574

**Parent comment**:
> >  Why does i2p (per the article) expect state sponsored attacks every February?
> 
> Because The Invisible Internet Project (I2P) allows government dissidents to communicate without the government oversight.  Censorship-resistant, peer-to-peer communication
> 
> > Where are those forming from, what does the regularity achieve?
> 
> At least PR China, Iran, Oman, Qatar, and Kuwait.  censor communication between dissidents.
> 
> >  How come the operators of giant (I’m assuming illegal) botnets are available t...

**This comment**:
> Sure, but why February and not the other 11 months?

---

### #19 (score: 3)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: Cider9986 | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108458

**Parent comment**:
> Ever tried to ban a botnet owner from a service they want to use?
> 
> It’s basically impossible. They have money, IPs, identities, anything you could possibly want to evade.

**This comment**:
> They are rich in regard to the tools needed to abuse services haha.

---

### #20 (score: 3)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: derstander | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107152

**Parent comment**:
> yeah, actually I wanted to see if this was possible at all. I managed to get around 3000 tokens/s on a ps2 with classic transformers, since the emotion engine is capable of 32 bit addresses, but it has like 32gb of ram. So I ran into the question of why was that fast and I couldn't get that speed even with small models, and the deal is that the instructions went right of the memory to the gpu and that's the main difference that does when a regular computer does inference: it has to request th...

**This comment**:
> *32MB of RAM (plus 4MB of video RAM and a little sound and IOP memory).

---

### #21 (score: 3)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: TechSquidTV | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107979

**Parent comment**:
> Yeah a ramdisk would probably work wonders. It's a shame Intel optane didn't became a standard, those type of workflows would be amazing for it.

**This comment**:
> Ahhh damn it. Intel! Come back!

---

### #22 (score: 3)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: hedgehog | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106631

**Parent comment**:
> Yeah I’ve often wondered why folks aren’t training two tier MoEs for VRAM + RAM. We already have designs for shared experts so it cannot be hard to implement a router that allocated 10x or 100x as often to “core” experts vs the “nice to have” experts. I suppose balancing during training is tricky but some sort of custom loss on the router layers should work.
> 
> I’ve also wondered why the routers aren’t training to be serially consistent so you can predict layers to swap into VRAM a few layers a...

**This comment**:
> I don't have links handy but there is active research in this area.

---

### #23 (score: 3)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: vlovich123 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106575

**Parent comment**:
> LLM speed is roughly <memory_bandwidth> / <model_size> tok/s.
> 
> DDR4 tops out about 27Gbs
> 
> DDR5 can do around 40Gbs
> 
> So for 70B model at 8 bit quant, you will get around 0.3-0.5 tokens per second using RAM alone.

**This comment**:
> Faster than the 0.2tok/s this approach manages

---

### #24 (score: 3)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: zozbot234 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106680

**Parent comment**:
> LLM speed is roughly <memory_bandwidth> / <model_size> tok/s.
> 
> DDR4 tops out about 27Gbs
> 
> DDR5 can do around 40Gbs
> 
> So for 70B model at 8 bit quant, you will get around 0.3-0.5 tokens per second using RAM alone.

**This comment**:
> Should be active param size, not model size.

---

### #25 (score: 3)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: xaskasdf | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107112

**Parent comment**:
> LLM speed is roughly <memory_bandwidth> / <model_size> tok/s.
> 
> DDR4 tops out about 27Gbs
> 
> DDR5 can do around 40Gbs
> 
> So for 70B model at 8 bit quant, you will get around 0.3-0.5 tokens per second using RAM alone.

**This comment**:
> yeah, actually, I'm bottlenecked af since my mobo got pcie3 only :(

---

### #26 (score: 3)

**Story**: Evidence of the bouba-kiki effect in naïve baby chicks
**Author**: downboots | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108576

**Parent comment**:
> No, language is still pretty close to arbitrary labels. The handful of tenuous common threads like the bouba-kiki effect don't change the overall picture that much. The simple fact that language varies as much as it does is sufficient to prove that it's only loosely bound to anything universal.

**This comment**:
> It must be bound to referent and medium

---

### #27 (score: 3)

**Story**: Evidence of the bouba-kiki effect in naïve baby chicks
**Author**: canjobear | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108460

**Parent comment**:
> I don't think it's abstract at all. Rub something sharp (anything from a stick to a phonograph needle) on an object and you'll directly transcribe its spatial frequency spectrum into an audio frequency spectrum.

**This comment**:
> Do you think it's obvious that a chick would understand that connection?

---

### #28 (score: 3)

**Story**: Evidence of the bouba-kiki effect in naïve baby chicks
**Author**: goodJobWalrus | **Depth**: 5 | **Signals**: thread death at depth 5, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107180

**Parent comment**:
> An interesting explanation that happens to be completely hallucinated. That line doesn't appear anywhere in either the play or the movie.

**This comment**:
> ha, ha, and they say AI does not hallucinate anymore!

---

### #29 (score: 3)

**Story**: Parse, Don't Validate and Type-Driven Design in Rust
**Author**: strawhatguy | **Depth**: 5 | **Signals**: thread death at depth 5, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107906

**Parent comment**:
> Coalton ( https://coalton-lang.github.io ) is the sort of thing I like: a Haskell-style language hosted inside a very dynamic one with good interop.

**This comment**:
> Yes it's quite the blend!

---

### #30 (score: 3)

**Story**: zclaw: personal AI assistant in under 888 KB, running on an ESP32
**Author**: sowbug | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106567

**Parent comment**:
> That's a crab. Get your crustaceans straight!

**This comment**:
> Thanks for looking out for us.

---

### #31 (score: 3)

**Story**: zclaw: personal AI assistant in under 888 KB, running on an ESP32
**Author**: grigio | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105779

**Parent comment**:
> The various *claws are just a pipe between LLM APIs and a bunch of other API/CLIs. Like you can have it listen via telegram or Whatsapp for a prompt you send. Like to generate some email or social post, which it sends to the LLM API. Get back a tool call that claw then makes to hit your email or social API. You could have it regularly poll for new emails or posts, generate a reply via some prompt, and send the reply.
> 
> The reason people were buying a separate Mac minis just to do open claw was...

**This comment**:
> yeah i still can't believe many people bought a mac mini just for the claw hype

---

### #32 (score: 3)

**Story**: zclaw: personal AI assistant in under 888 KB, running on an ESP32
**Author**: codazoda | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105564

**Parent comment**:
> I don't fully get it either. At least agents build stuff, claws just run around pretending to be alive?

**This comment**:
> They do build things. The same things.

---

### #33 (score: 3)

**Story**: zclaw: personal AI assistant in under 888 KB, running on an ESP32
**Author**: fragmede | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105881

**Parent comment**:
> Why do you have so many? eWaste..

**This comment**:
> I need 1, but they come in packs of 10.

---

### #34 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: 127 | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105534

**Parent comment**:
> Ok but this is how the market is supposed to work. If the incumbents aren't doing what their customers want, then competitors can rise and fill the gap and compete.
> 
> This isn't a shortcoming, it's a competitive market working as intended.

**This comment**:
> Only if you don't want or need any geopolitical gradient at all.

---

### #35 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: yellowapple | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108529

**Parent comment**:
> That's a small b, that's gigabit.
> And that matches e.g. a "crucial pro udimm 64GB kit ddr4-3200 cl22-22-22 2Rx8" being at about 100$ a year ago.
> 
> Currently they sell here in Germany for 409€ each, that's 6.25€ for half of each of the 16Gbit chips on that kit.

**This comment**:
> > That's a small b, that's gigabit.
> 
> That explains it, thanks!

---

### #36 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: Snoozus | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108822

**Parent comment**:
> Western European countries got dominant, then got arrogant, letting the USA eat their lunch.
> 
> USA got dominant, got arrogant, letting China eat their lunch.
> 
> China is indeed getting dominant.  They will get arrogant one day.  Meanwhile, Western Europe and the USA are still very good places to live.

**This comment**:
> Exactly, there are no losers here, everyone gets lunch.

---

### #37 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: ceejayoz | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107636

**Parent comment**:
> History is littered with the corpses of those slaughtered by the millions in the name of great leader’s 5 year plans.
> 
> https://en.wikipedia.org/wiki/Great_Leap_Forward

**This comment**:
> And you don’t think short term profit chasing has a death count?

---

### #38 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: rjh29 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108810

**Parent comment**:
> Can we please stop with this irritatingly persistent myth? AI companies, at least the big ones, do not sell inference at a loss - far from it. This has been debunked and explained many times and yet it keeps being repeated.
> 
> The numbers aren't public but most guesses I've heard are that Anthropic's markup is around 50% on average, and that if considered in isolation, most models are profitable overall. The constant losses are instead due to training the next models, which will also eventually...

**This comment**:
> You talk about myths and then quote vague guesses of 50% without sourcing.

---

### #39 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: throwaway290 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108591

**Parent comment**:
> You mean the US? Yeah, they are.

**This comment**:
> sorry forgot Samsung is a US company

---

### #40 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: throwaway290 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107558

**Parent comment**:
> is CXMT attacking every contry?

**This comment**:
> its owner and boss
> 
> but sure if they are ordered to they will, good point too;)

---

### #41 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: estimator7292 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47103056

**Parent comment**:
> The market doing what it's supposed to do does not negate that the market segment has only been left open because of overly myopic businesses.

**This comment**:
> That's what modern capitalism is and it's bad for everyone

---

### #42 (score: 3)

**Story**: CXMT has been offering DDR4 chips at about half the prevailing market rate
**Author**: ta9000 | **Depth**: 5 | **Signals**: thread death at depth 5, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47107202

**Parent comment**:
> "Why is nobody berating China?" is my favorite oft-repeated refrain on HN.

**This comment**:
> They’re too busy berating the US.

---

### #43 (score: 3)

**Story**: Coccinelle: Source-to-source transformation tool
**Author**: cpeterso | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47103610

**Parent comment**:
> I found that font extremely hard to read for some reason (on my phone). So I gave up. Maybe due to you using a monospace font for non-code?
> 
> But I believe smalltalk represented code as functions in a database somehow, so maybe that is worth looking at.

**This comment**:
> > So I gave up.
> 
> Try your mobile browser’s reader view mode.

---

### #44 (score: 3)

**Story**: Coccinelle: Source-to-source transformation tool
**Author**: conartist6 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47101675

**Parent comment**:
> What's BABLR?

**This comment**:
> The mission is the same as OpenRewrite: parse and transform any code.

---

### #45 (score: 3)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: satvikpendem | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105704

**Parent comment**:
> > Hydrogen is such a terrible idea it was never getting off the ground.
> 
> It's coming from Toyota because Toyota can't wrap its head around not making engines. Ironically, the place hydrogen might work is airplanes where the energy density of batteries doesn't work.

**This comment**:
> What does this mean? They have electric vehicles too.

---

### #46 (score: 3)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: L-four | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105441

**Parent comment**:
> Why is it such a terrible idea? In theory you can generate it via electrolysis in places with plentiful renewable energy, and then you've got a very high-density, lightweight fuel. On the surface, it seems ideal for things like cars or planes where vehicle weight matters. Batteries are huge and heavy and nowhere near as energy dense as gasoline.

**This comment**:
> The cheapest way to make hydrogen is to use fossil fuels.

---

### #47 (score: 3)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: smcin | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105671

**Parent comment**:
> $15,000 worth of fuel card sounds generous until you find that hydrogen stations have jacked up prices to $36/kg.

**This comment**:
> Full tank capacity of a Mirai is ~5 kg / (120 liters in volume).

---

### #48 (score: 3)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: DangitBobby | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105899

**Parent comment**:
> > It is just so much simpler with electricity.
> 
> Yet the market still thinks differently. Lots of countries still keep subsidizing EV despite them already being mature technology for such a long time.
> 
> We didn't have to subsidize the smart phone to make it successful, we shouldn't have to subsidize electric cars either.

**This comment**:
> ICE love is cultural, and there's a bunch of FUD from entrenched interests.

---

### #49 (score: 3)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: XorNot | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105316

**Parent comment**:
> Gaseous form is a problem, but have you seen the Fraunhofer POWERPASTE? I was optimistic when the news was first announced, but that was a decade ago and of course it's not widely used.

**This comment**:
> At that point you're just building a weird battery storage system again though.

---

### #50 (score: 3)

**Story**: Toyota Mirai hydrogen car depreciation: 65% value loss in a year
**Author**: peterfirefly | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47104926

**Parent comment**:
> There's only... well, 51 of them. If you're lucky, you're near one of the 42 that are actually online and available for fueling (as of this comment).
> 
> Stations running out of fuel and stations going offline for hardware failures runs rampant.
> 
> Oh, and some stations might not be able to provide the highest pressure H2, so you might be stuck taking an 85% tank fill... and at nearly $30/kg and a 5.6kg (full) tank, that's an expensive fill.
> 
> https://h2-ca.com/

**This comment**:
> And they are not even supposed to explode anymore!

---

### #51 (score: 3)

**Story**: Canvas_ity: A tiny, single-header <canvas>-like 2D rasterizer for C++
**Author**: capisce | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108908

**Parent comment**:
> It is a common pattern among those that don't want to learn build systems, which isn't exactly the same.

**This comment**:
> Or among those who want to support any build system

---

### #52 (score: 3)

**Story**: EDuke32 – Duke Nukem 3D (Open-Source)
**Author**: fragmede | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105226

**Parent comment**:
> It was a wonderful collection of rage inducing weapons: pipe bombs, laser trip mines, shrink ray (then step on them for the kill), freeze gun (any hit shatters for the kill), and the BFG.
> 
> We had LAN parties and would play for hours on end with custom maps we had built or downloaded.

**This comment**:
> Hail to the king, baby!

---

### #53 (score: 3)

**Story**: EDuke32 – Duke Nukem 3D (Open-Source)
**Author**: gloflo | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106150

**Parent comment**:
> Make up for what? I don’t get it.

**This comment**:
> For illegally copying software.

---

### #54 (score: 3)

**Story**: EDuke32 – Duke Nukem 3D (Open-Source)
**Author**: nurettin | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106844

**Parent comment**:
> > then step on them for the kill
> 
> I heard the sound effect of that when I read you're comment.

**This comment**:
> Yeah it sounded like what I imagine pressing on a cardboard of eggs would.

---

### #55 (score: 3)

**Story**: EDuke32 – Duke Nukem 3D (Open-Source)
**Author**: throwatdem12311 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105082

**Parent comment**:
> Ashes 2063 looks awesome thank you for sharing!
> 
> I downloaded the AshesStandalone_V1_51.zip file, but it looks like it only contains the windows executable. For our linux friends, unzip it, install gzdoom, and then run this command inside the "Resources" folder to play it on linux:
> 
> > gzdoom -config gzdoom-ashes.ini -iwad freedoom-0.12.1/freedoom2.wad -file AshesSAMenu.pk3 lightmodepatch.pk3 Ashes2063Enriched2_23.pk3 Ashes2063EnrichedFDPatch.pk3   +logfile log.txt

**This comment**:
> Don’t forgot to check out Ashes Hard Reset and Afterglow as well.

---

### #56 (score: 3)

**Story**: Be wary of Bluesky
**Author**: esseph | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47097276

**Parent comment**:
> Most people were happy with Twitter as well

**This comment**:
> That's a very strong statement to make.

---

### #57 (score: 3)

**Story**: Be wary of Bluesky
**Author**: verdverm | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106641

**Parent comment**:
> what's an email alias? (in the sense that they would know you were using one)

**This comment**:
> I believe they mean the `email+alias@domain.com` (I forget the order)

---

### #58 (score: 3)

**Story**: Be wary of Bluesky
**Author**: red75prime | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108356

**Parent comment**:
> I salute the solidarity /s
> 
> Joking aside, I think what we see in the larger scheme is a fracturing of social media. More choice, more competition.
> 
> This is a good thing

**This comment**:
> More choice, more competition, more echochambers.

---

### #59 (score: 3)

**Story**: A16z partner says that the theory that we’ll vibe code everything is wrong
**Author**: bigbuppo | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47106685

**Parent comment**:
> > Vibe Coding has its uses and I'm sure that'll expand, but the idea of it replacing domain experts is outright laughable.
> 
> I don't think that's the argument. The argument I'm seeing most is that most of us SWEs will become obsolete once the agentic tools become good enough to allow domain experts to fully iterate on solutions on their own.

**This comment**:
> How do you get domain experts?

---

### #60 (score: 3)

**Story**: A16z partner says that the theory that we’ll vibe code everything is wrong
**Author**: rsrsrs86 | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47104487

**Parent comment**:
> There is a lot of work that goes on before even reaching the point to write code.
> 
> For example, being able to vibecode a UI wireframe instead of being blocked for 2 sprints by your UI/UX team or templating an alpha to gauge customer interest in 1 week instead of 1 quarter is a massive operational improvement.
> 
> Of course these aren't completed products, but customers in most cases can accept such performance in the short-to-medium term or if it is part of an alpha.
> 
> This is why I keep repeatin...

**This comment**:
> As a researcher in formal methods, I totally get you

---

### #61 (score: 3)

**Story**: What not to write on your security clearance form (1988)
**Author**: pousada | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47103062

**Parent comment**:
> He used to (maybe still does) have a page where he talked about turning down millions of dollars for it.

**This comment**:
> See the link above.
> He’s willing to part with it for 10 million

---

### #62 (score: 3)

**Story**: What not to write on your security clearance form (1988)
**Author**: godelski | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47105507

**Parent comment**:
> It seems to me that if you lie and get the clearance, it is better than being honest and getting NACKed. Maybe morally dubious, but there's financial incentive and motivation for having a clearance.

**This comment**:
> I think you need to reread my comment... you seem to have misunderstandings...

---

### #63 (score: 3)

**Story**: What not to write on your security clearance form (1988)
**Author**: p1anecrazy | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47103415

**Parent comment**:
> Here you go:
> https://yarchive.net/risks/mongrel.html

**This comment**:
> What a wholesome guy. Thanks for the read

---

### #64 (score: 3)

**Story**: What not to write on your security clearance form (1988)
**Author**: piskov | **Depth**: 3 | **Signals**: thread death at depth 3, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108067

**Parent comment**:
> I think the motivation and experience of those camps were quite different

**This comment**:
> Yeah, let’s call that involuntary race-based detention a retreat.

---

### #65 (score: 3)

**Story**: Iranian Students Protest as Anger Grows
**Author**: nickff | **Depth**: 4 | **Signals**: thread death at depth 4, short reply in thread
**Link**: https://news.ycombinator.com/item?id=47108797

**Parent comment**:
> > the economy is terrible and getting worse
> 
> we could always stop punishing the people of Iran for their government...

**This comment**:
> And the government could stop murdering people.

---

### #66 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: haolez | **Depth**: 1 | **Signals**: 23 direct replies
**Link**: https://news.ycombinator.com/item?id=47107091

> > Notice the language: “deeply”, “in great details”, “intricacies”, “go through everything”. This isn’t fluff. Without these words, Claude will skim. It’ll read a file, see what a function does at the signature level, and move on. You need to signal that surface-level reading is not acceptable.
> 
> This makes no sense to my intuition of how an LLM works. It's not that I don't believe this works, but my mental model doesn't capture why asking the model to read the content "more deeply" will have any impact on whatever output the LLM generates.

---

### #67 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: jamesmcq | **Depth**: 1 | **Signals**: 10 direct replies
**Link**: https://news.ycombinator.com/item?id=47107077

> This all looks fine for someone who can't code, but for anyone with even a moderate amount of experience as a developer all this planning and checking and prompting and orchestrating is far more work than just writing the code yourself.
> 
> There's no winner for "least amount of code written regardless of productivity outcomes.", except for maybe Anthropic's bank account.

---

### #68 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: strix_varius | **Depth**: 1 | **Signals**: 1 escalation keyword(s)
**Link**: https://news.ycombinator.com/item?id=47108685

> The baffling part of the article is all the assertions about how this is unique, novel, not the typical way people are doing this etc.
> 
> There are whole products wrapped around this common workflow already (like Augment Intent).

---

### #69 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: majormajor | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108530

**Parent comment**:
> That’s because it’s superstition.
> 
> Unless someone can come up with some kind of rigorous statistics on what the effect of this kind of priming is it seems no better than claiming that sacrificing your first born will please the sun god into giving us a bountiful harvest next year.
> 
> Sure, maybe this supposed deity really is this insecure and needs a jolly good pep talk every time he wakes up. or maybe you’re just suffering from magical thinking that your incantations had any effect on the rand...

**This comment**:
> > If it did work, well, the oldest trick in computer science is writing compilers, i suppose we will just have to write an English to pedantry compiler.
> 
> "Add tests to this function" for GPT-3.5-era models was much less effective than "you are a senior engineer. add tests for this function. as a good engineer, you should follow the patterns used in these other three function+test examples, using this framework and mocking lib." In today's tools, "add tests to this function" results in a bunch of initial steps to look in common places to see if that additional context already exists, and then pull it in based on what it finds. You can see it in the output the tools spit out while "thinking."
> 
> So I'm 90% sure this is already happening on some level.

---

### #70 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: rzmmm | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108786

**Parent comment**:
> That’s because it’s superstition.
> 
> Unless someone can come up with some kind of rigorous statistics on what the effect of this kind of priming is it seems no better than claiming that sacrificing your first born will please the sun god into giving us a bountiful harvest next year.
> 
> Sure, maybe this supposed deity really is this insecure and needs a jolly good pep talk every time he wakes up. or maybe you’re just suffering from magical thinking that your incantations had any effect on the rand...

**This comment**:
> I think "understand this directory deeply" just gives more focus for the instruction. So it's like "burn more tokens for this phase than you normally would".

---

### #71 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: harrall | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107884

**Parent comment**:
> Its a wild time to be in software development. Nobody(1) actually knows what causes LLMs to do certain things, we just pray the prompt moves the probabilities the right way enough such that it mostly does what we want. This used to be a field that prided itself on deterministic behavior and reproducibility.
> 
> Now? We have AGENTS.md files that look like a parent talking to a child with all the bold all-caps, double emphasis, just praying that's enough to be sure they run the commands you want t...

**This comment**:
> It’s like playing a fretless instrument to me.
> 
> Practice playing songs by ear and after 2 weeks, my brain has developed an inference model of where my fingers should go to hit any given pitch.
> 
> Do I have any idea how my brain’s model works? No! But it tickles a different part of my brain and I like it.

---

### #72 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: basch | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108660

**Parent comment**:
> these sort-of-lies might help:
> 
> think of the latent space inside the model like a topological map, and when you give it a prompt, you're dropping a ball at a certain point above the ground, and gravity pulls it along the surface until it settles.
> 
> caveat though, thats nice per-token, but the signal gets messed up by picking a token from a distribution, so each token you're regenerating and re-distorting the signal. leaning on language that places that ball deep in a region that you want to be...

**This comment**:
> My mental model for them is plinko boards.  Your prompt changes the spacing between the nails to increase the probability in certain directions as your chip falls down.

---

### #73 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: noduerme | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108001

**Parent comment**:
> these sort-of-lies might help:
> 
> think of the latent space inside the model like a topological map, and when you give it a prompt, you're dropping a ball at a certain point above the ground, and gravity pulls it along the surface until it settles.
> 
> caveat though, thats nice per-token, but the signal gets messed up by picking a token from a distribution, so each token you're regenerating and re-distorting the signal. leaning on language that places that ball deep in a region that you want to be...

**This comment**:
> Hah! Reading this, my mind inverted it a bit, and I realized ... it's like the claw machine theory of gradient descent. Do you drop the claw into the deepest part of the pile, or where there's the thinnest layer, the best chance of grabbing something specific? Everyone in everu bar has a theory about claw machines. But the really funny thing that unites LLMs with claw machines is that the biggest question is always whether they dropped the ball on purpose.
> 
> The claw machine is also a sort-of-lie, of course. Its main appeal is that it offers the illusion of control. As a former designer and coder of online slot machines...  totally spin off into pages on this analogy, about how that illusion gets you to keep pulling the lever... but the geographic rendition you gave is sort of priceless when you start making the comparison.

---

### #74 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: bpodgursky | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107327

**Parent comment**:
> Yeah, it's definitely a strange new world we're in, where I have to "trick" the computer into cooperating. The other day I told Claude "Yes you can", and it went off and did something it just said it couldn't do!

**This comment**:
> You bumped the token predictor into the latent space where it knew what it was doing : )

---

### #75 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: red_hare | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107836

**Parent comment**:
> Can I ask how you annotate the feedback for it? Just with inline comments like `# This should be changed to X`?
> 
> The author mentions annotations but doesn't go into detail about how to feed the annotations to Claude.

**This comment**:
> Slidev is markdown, so i do it in html comments. Usually something like:
> 
>     <!-- TODOCLAUDE: Split this into a two-cols-title, divide the examples between -->
> 
> or
> 
>     <!-- TODOCLAUDE: Use clipart skill to make an image for this slide -->
> 
> And then, when I finish annotating I just say: "Address all the TODOCLAUDEs"

---

### #76 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: red_hare | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107882

**Parent comment**:
> is your skill open source

**This comment**:
> Not yet... but also I'm not sure it makes a lot of sense to be open source. It's super specific to how I like to build slide decks and to my personal lecture style.
> 
> But it's not hard to build one. The key for me was describing, in great detail:
> 
> 1. How I want it to read the source material (e.g., H1 means new section, H2 means at least one slide, a link to an example means I want code in the slide)
> 
> 2. How to connect material to layouts (e.g., "comparison between two ideas should be a two-cols-title," "walkthrough of code should be two-cols with code on right," "learning objectives should be side-title align:left," "recall should be side-title align:right")
> 
> Then the workflow is:
> 
> 1. Give all those details and have it do a first pass.
> 
> 2. Give tons of feedback.
> 
> 3. At the end of the session, ask it to "make a skill."
> 
> 4. Manually edit the skill so that you're happy with the examples.

---

### #77 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: tyleo | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107199

**Parent comment**:
> I really don't understand why there are so many comments like this.
> 
> Yesterday I had Claude write an audit logging feature to track all changes made to entities in my app. Yeah you get this for free with many frameworks, but my company's custom setup doesn't have it.
> 
> It took maybe 5-10 minutes of wall-time to come up with a good plan, and then ~20-30 min for Claude implement, test, etc.
> 
> That would've taken me at least a day, maybe two. I had 4-5 other tasks going on in other tabs while I wa...

**This comment**:
> Same here. I did bounce off these tools a year ago. They just didn't work for me 60% of the time. I learned a bit in that initial experience though and walked away with some tasks ChatGPT could replace in my workflow. Mainly replacing scripts and reviewing single files or functions.
> 
> Fast forward to today and I tried the tools again--specifically Claude Code--about a week ago. I'm blown away. I've reproduced some tools that took me weeks at full-time roles in a single day. This is while reviewing every line of code. The output is more or less what I'd be writing as a principal engineer.

---

### #78 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: jamesmcq | **Depth**: 3 | **Signals**: 1 escalation keyword(s)
**Link**: https://news.ycombinator.com/item?id=47107247

**Parent comment**:
> I really don't understand why there are so many comments like this.
> 
> Yesterday I had Claude write an audit logging feature to track all changes made to entities in my app. Yeah you get this for free with many frameworks, but my company's custom setup doesn't have it.
> 
> It took maybe 5-10 minutes of wall-time to come up with a good plan, and then ~20-30 min for Claude implement, test, etc.
> 
> That would've taken me at least a day, maybe two. I had 4-5 other tasks going on in other tabs while I wa...

**This comment**:
> Trust me I'm very impressed at the progress AI has made, and maybe we'll get to the point where everything is 100% correct all the time and better than any human could write. I'm skeptical we can get there with the LLM approach though.
> 
> The problem is LLMs are great at simple implementation, even large amounts of simple implementation, but I've never seen it develop something more than trivial correctly. The larger problem is it's very often subtly but hugely wrong. It makes bad architecture decisions, it breaks things in pursuit of fixing or implementing other things. You can tell it has no concept of the "right" way to implement something. It very obviously lacks the "senior developer insight".
> 
> Maybe you can resolve some of these with large amounts of planning or specs, but that's the point of my original comment - at what point is it easier/faster/better to just write the code yourself? You don't get a prize for writing the least amount of code when you're just writing specs instead.

---

### #79 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: skydhash | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107569

**Parent comment**:
> I really don't understand why there are so many comments like this.
> 
> Yesterday I had Claude write an audit logging feature to track all changes made to entities in my app. Yeah you get this for free with many frameworks, but my company's custom setup doesn't have it.
> 
> It took maybe 5-10 minutes of wall-time to come up with a good plan, and then ~20-30 min for Claude implement, test, etc.
> 
> That would've taken me at least a day, maybe two. I had 4-5 other tasks going on in other tabs while I wa...

**This comment**:
> > Yesterday I had Claude write an audit logging feature to track all changes made to entities in my app. Yeah you get this for free with many frameworks, but my company's custom setup doesn't have it.
> 
> But did you truly think about such feature? Like guarantees that it should follow (like how do it should cope with entities migration like adding a new field) or what the cost of maintaining it further down the line. This looks suspiciously like drive-by PR made on open-source projects.
> 
> > That would've taken me at least a day, maybe two.
> 
> I think those two days would have been filled with research, comparing alternatives, questions like "can we extract this feature from framework X?", discussing ownership and sharing knowledge,.. Jumping on coding was done before LLMs, but it usually hurts the long term viability of the project.
> 
> Adding code to a project can be done quite fast (hackatons,...), ensuring quality is what slows things down in any any well functioning team.

---

### #80 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: apsurd | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108536

**Parent comment**:
> Huge +1. This loop consistently delivers great results for my vibe coding.
> 
> The “easy” path of “short prompt declaring what I want” works OK for simple tasks but consistently breaks down for medium to high complexity tasks.

**This comment**:
> Can you help me understand the difference between "short prompt for what I want (next)" vs medium to high complexity tasks?
> 
> What i mean is, in practice, how does one even get to a a high complexity task? What does that look like? Because isn't it more common that one sees only so far ahead?

---

### #81 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: throwup238 | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107429

**Parent comment**:
> Looks good. Question - is it always better to use a monorepo in this new AI world? Vs breaking your app into separate repos? At my company we have like 6 repos all separate nextjs apps for the same user base. Trying to consolidate to one as it should make life easier overall.

**This comment**:
> It really depends but there’s nothing stopping you from just creating a separate folder with the cloned repositories (or worktrees) that you need and having a root CLAUDE.md file that explains the directory structure and referencing the individual repo CLAUDE.md files.

---

### #82 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: oa335 | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107407

**Parent comment**:
> Looks good. Question - is it always better to use a monorepo in this new AI world? Vs breaking your app into separate repos? At my company we have like 6 repos all separate nextjs apps for the same user base. Trying to consolidate to one as it should make life easier overall.

**This comment**:
> Just put all the repos in all in one directory yourself.  In my experience that works pretty well.

---

### #83 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: chickensong | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107478

**Parent comment**:
> Looks good. Question - is it always better to use a monorepo in this new AI world? Vs breaking your app into separate repos? At my company we have like 6 repos all separate nextjs apps for the same user base. Trying to consolidate to one as it should make life easier overall.

**This comment**:
> AI is happy to work with any directory you tell it to. Agent files can be applied anywhere.

---

### #84 (score: 2)

**Story**: How I use Claude Code: Separation of planning and execution
**Author**: silversmith | **Depth**: 4 | **Signals**: thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108657

**Parent comment**:
> Sufficiently advanced technology has become like magic: you have to prompt the electronic genie with the right words or it will twist your wishes.

**This comment**:
> Light some incense, and you too can be a dystopian space tech support, today! Praise Omnissiah!

---

### #85 (score: 2)

**Story**: Japanese Woodblock Print Search
**Author**: PacificSpecific | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108443

**Parent comment**:
> I was going to ask what the other thread was but you made it easy!
> 
> The Rediscovery of 103 Hokusai Lost Sketches (2021) - https://news.ycombinator.com/item?id=47030387 - Feb 2026 (8 comments)

**This comment**:
> Ah I should have posted the link. Glad you were able to find it and thanks for digging it up!

---

### #86 (score: 2)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: illusive4080 | **Depth**: 1 | **Signals**: 6 direct replies
**Link**: https://news.ycombinator.com/item?id=47107738

> Why does Discord allow a server for a botnet owner?

---

### #87 (score: 2)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: bee_rider | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108736

**Parent comment**:
> Ever tried to ban a botnet owner from a service they want to use?
> 
> It’s basically impossible. They have money, IPs, identities, anything you could possibly want to evade.

**This comment**:
> It would be pretty funny if the age verification stuff blocked some of these folks.

---

### #88 (score: 2)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: charcircuit | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108731

**Parent comment**:
> Ever tried to ban a botnet owner from a service they want to use?
> 
> It’s basically impossible. They have money, IPs, identities, anything you could possibly want to evade.

**This comment**:
> If you just look at the messages in those kinds of discords. It's blatant. They aren't even trying to hide it.

---

### #89 (score: 2)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: conradev | **Depth**: 4 | **Signals**: thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108830

**Parent comment**:
> No. They should not try to survive such attacks. The best defense to a temporary attack is often to pull the plug.  Better than than potentially expose users.  When there are 10x as many bad nodes as good, the base protection of any anonymity network is likely compromised. Shut down, survive, and return once the attacker has moved on.

**This comment**:
> This is why Tor is centralized, so that they can take action like cutting out malicious nodes if needed. It’s decentralized in the sense that anyone can participate by default.

---

### #90 (score: 2)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: braingravy | **Depth**: 4 | **Signals**: thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108494

**Parent comment**:
> This answer is missing the key "regularity" part of their questions, which I would love to know more about.

**This comment**:
> That’s a great question… Currently we’re in the main Chinese holiday period with the Lunar New Year/Spring Festival/Chinese New Year, so perhaps people traveling back home from foreign lands might use the service more during this time?

---

### #91 (score: 2)

**Story**: A Botnet Accidentally Destroyed I2P
**Author**: xmcp123 | **Depth**: 5 | **Signals**: thread death at depth 5
**Link**: https://news.ycombinator.com/item?id=47108246

**Parent comment**:
> Why would an attacker move on if it can maintain a successful DoS attack forever?

**This comment**:
> Because botnets are mostly there to make money nowadays. Or owned by state actors.
> 
> Either way, it’s  opportunity cost.

---

### #92 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: teo_zero | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108777

**Parent comment**:
> Cost wise it does not seem very effective. .5 token / sec (the optimized one) is 3600 tokens an hour, which costs about 200-300 watts for an active 3090+system. Running 3600 tokens on open router @.4$ for llama 3.1 (3.3 costs less), is about $0,00144. That money buys you about 2-3 watts (in the Netherlands).
> 
> Great achievement for privacy inference nonetheless.

**This comment**:
> I think we use different units. In my system there are 3600 seconds per hour, and watts measure power.

---

### #93 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: Aerroon | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108476

**Parent comment**:
> Cost wise it does not seem very effective. .5 token / sec (the optimized one) is 3600 tokens an hour, which costs about 200-300 watts for an active 3090+system. Running 3600 tokens on open router @.4$ for llama 3.1 (3.3 costs less), is about $0,00144. That money buys you about 2-3 watts (in the Netherlands).
> 
> Great achievement for privacy inference nonetheless.

**This comment**:
> Something to consider is that input tokens have a cost too. They are typically processed much faster than output tokens. If you have long conversations then input tokens will end up being a significant part of the cost.
> 
> It probably won't matter much here though.

---

### #94 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: charcircuit | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47108875

**Parent comment**:
> I wonder too, DMA plays a huge role in most older gaming consoles when the CPUs were far more sluggish.
> 
> Perhaps that's what made them think to try.
> 
> Perhaps the current batch of smart memory cards which on the PS2 I believe have quite complex DMA capabilities to stream from the SD card game data.

**This comment**:
> Why not the PS5? That's when games started streaming assets straight from the NVME SSD to the GPU. In this case the assets are weights.

---

### #95 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: tgrowazay | **Depth**: 3 | **Signals**: 5 direct replies
**Link**: https://news.ycombinator.com/item?id=47106464

**Parent comment**:
> I didn't really understand the performance table until I saw the top ones were 8B models.
> 
> But 5 seconds / token is quite slow yeah. I guess this is for low ram machines? I'm pretty sure my 5950x with 128 gb ram can run this faster on the CPU with some layers / prefill on the 3060 gpu I have.
> 
> I also see that they claim the process is compute bound at 2 seconds/token, but that doesn't seem correct with a 3090?

**This comment**:
> LLM speed is roughly <memory_bandwidth> / <model_size> tok/s.
> 
> DDR4 tops out about 27Gbs
> 
> DDR5 can do around 40Gbs
> 
> So for 70B model at 8 bit quant, you will get around 0.3-0.5 tokens per second using RAM alone.

---

### #96 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: svnt | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47106640

**Parent comment**:
> Yeah I’ve often wondered why folks aren’t training two tier MoEs for VRAM + RAM. We already have designs for shared experts so it cannot be hard to implement a router that allocated 10x or 100x as often to “core” experts vs the “nice to have” experts. I suppose balancing during training is tricky but some sort of custom loss on the router layers should work.
> 
> I’ve also wondered why the routers aren’t training to be serially consistent so you can predict layers to swap into VRAM a few layers a...

**This comment**:
> Maybe I am misunderstanding something but:
> 
> 1) This is basically the intention of several recent MoE models: keep particular generally useful experts hot in VRAM.
> 
> 2) Unless you can swap layers in faster than you consume them there is no point to predicting layers (what does this even really mean? did you mean predicting experts?).
> 
> It seems at the moment the best you can do is keep experts and layers more likely to be used for a given query in VRAM and offload the rest, but this is work-dependent.

---

### #97 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: xaskasdf | **Depth**: 3 | **Signals**: thread death at depth 3
**Link**: https://news.ycombinator.com/item?id=47107845

**Parent comment**:
> My impression is that that is limited to assets and really needs to fit into the DirectX framework. From what I can tell, the gpu-nvme-direct is mostly similar to https://github.com/enfiskutensykkel/ssd-gpu-dma and https://github.com/ZaidQureshi/bam

**This comment**:
> Actually this idea was fueled by those since I went to check if there was anything near to what I wanted to achieve, pretty useful tho

---

### #98 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: superkuh | **Depth**: 4 | **Signals**: thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108575

**Parent comment**:
> > I don't have 30k bucks to spare on a gpu :(
> 
> Do you have $2/hr to rent an RTX 6000 96GB or $5/hr for B200 180GB on the cloud?

**This comment**:
> I'd rather not give money to scalper barons if I can avoid it. Fab capacity is going to that for rental rather than hardware for humans.

---

### #99 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: fc417fc802 | **Depth**: 4 | **Signals**: thread death at depth 4
**Link**: https://news.ycombinator.com/item?id=47108321

**Parent comment**:
> 3000 tokens per sec on 32 mb Ram?

**This comment**:
> fast != practical
> 
> You can get lots of tokens per second on the CPU if the entire network fits in L1 cache. Unfortunately the sub 64 kiB model segment isn't looking so hot.
> 
> But actually ... 3000? Did GP misplace one or two zeros there?

---

### #100 (score: 2)

**Story**: Show HN: Llama 3.1 70B on a single RTX 3090 via NVMe-to-GPU bypassing the CPU
**Author**: wtallis | **Depth**: 5 | **Signals**: thread death at depth 5
**Link**: https://news.ycombinator.com/item?id=47107352

**Parent comment**:
> Channels matter a lot, quad channel ddr4 is going to beat ddr5 in dual channel most of the time.

**This comment**:
> Four channels of DDR4-3200 vs two channels of DDR5-6400 (four subchannels) should come out pretty close. I don't see any reason why the DDR4 configuration would be consistently faster; you might have more bank groups on DDR4, but I'm not sure that would outweigh other factors like the topology and bandwidth of the interconnects between the memory controller and the CPU cores.

---
